**Проект: Система рекомендаций фильмов на основе контентных характеристик**
https://www.kaggle.com/datasets/tmdb/tmdb-movie-metadata?resource=download&select=tmdb_5000_movies.csv 
### **Описание проекта**
Этот проект реализует рекомендательную систему фильмов, основанную на анализе их содержательных характеристик (Content-Based Filtering). В основе лежит обработка информации о фильмах из датасета TMDB 5000, включая жанры, ключевые слова, актерский состав и описание. 

### **Цель проекта**
Создать эффективную систему рекомендаций фильмов, которая анализирует контент фильмов и предлагает пользователю наиболее схожие варианты. Система будет работать на основе текстового сходства между описаниями фильмов, жанрами, ключевыми словами, актёрами и режиссёрами.

### **Методология**
1. **Предобработка данных** – загрузка датасета, очистка, выбор ключевых характеристик.
2. **Обработка текстовых данных** – лемматизация, удаление стоп-слов, объединение признаков.
3. **Формирование векторного представления** – преобразование текстовых данных в числовой формат (TF-IDF, CountVectorizer).
4. **Расчёт схожести фильмов** – применение метрик косинусного расстояния.
5. **Создание системы рекомендаций** – выбор наиболее схожих фильмов.

### **Датасеты**
- `tmdb_5000_credits.csv` – содержит информацию об актёрском составе и съёмочной группе.
- `tmdb_5000_movies.csv` – содержит жанры, ключевые слова, описание фильма, бюджет и другие данные.

---

### **Этап 1: Загрузка и первичная обработка данных**

#### **Описание**
На этом этапе выполняется загрузка двух датасетов, их объединение и выбор ключевых столбцов для дальнейшего анализа.

#### **Код**
```python
import pandas as pd
import numpy as np
import ast

# Загрузка данных
info = pd.read_csv('tmdb_5000_credits.csv')
movies = pd.read_csv('tmdb_5000_movies.csv')

# Объединение данных по названию фильма
movies = movies.merge(info, on='title')

# Вывод информации о датасете
movies.info()

# Выбор ключевых столбцов
movies = movies[['movie_id', 'title', 'release_date', 'overview', 'genres', 'keywords', 'cast', 'crew']]

# Удаление пропущенных значений
movies.dropna(inplace=True)
```

#### **Объяснение кода**
1. Загружаются файлы `tmdb_5000_credits.csv` и `tmdb_5000_movies.csv`.
2. Осуществляется объединение двух таблиц по названию фильма (`title`).
3. Выбираются ключевые столбцы:
   - `movie_id` – идентификатор фильма,
   - `title` – название фильма,
   - `release_date` – дата выхода,
   - `overview` – описание,
   - `genres` – список жанров,
   - `keywords` – ключевые слова,
   - `cast` – актерский состав,
   - `crew` – съёмочная группа.
4. Удаляются записи с пропущенными значениями.

### **Ожидаемый результат**
- Данные загружены и объединены.
- Выбраны только необходимые столбцы.
- Удалены записи с пропущенными значениями.

---

### **Этап 2: Обработка текстовых данных**

#### **Описание**
На данном этапе выполняется разбор JSON-структур, обработка списка жанров, ключевых слов, актёрского состава и съёмочной группы. Также удаляются пробелы из названий и очищается дата выхода фильма.

#### **Код**
```python
# Функция для извлечения значений из JSON-структуры
def convert(text):
    L = []
    for i in ast.literal_eval(text):
        L.append(i['name'])
    return L

# Функция для извлечения первых трёх актёров
def convert_for_cast(text):
    L = []
    counter = 0
    for i in ast.literal_eval(text):
        if counter < 3:
            L.append(i['name'])
        counter += 1
    return L

# Функция для извлечения режиссёра
def convert_for_crew(text):
    L = []
    for i in ast.literal_eval(text):
        if i['job'] == 'Director':
            L.append(i['name'])
    return L

# Применение функций к соответствующим столбцам
movies['genres'] = movies['genres'].apply(convert)
movies['keywords'] = movies['keywords'].apply(convert)
movies['cast'] = movies['cast'].apply(convert_for_cast)
movies['crew'] = movies['crew'].apply(convert_for_crew)

# Функция для удаления пробелов в названиях
def delete_space(L):
    return [i.replace(" ", "") for i in L]

# Очистка названий от пробелов
movies['cast'] = movies['cast'].apply(delete_space)
movies['crew'] = movies['crew'].apply(delete_space)
movies['genres'] = movies['genres'].apply(delete_space)
movies['keywords'] = movies['keywords'].apply(delete_space)

# Преобразование даты выхода в год
movies['release_date'] = pd.to_datetime(movies['release_date']).dt.year
```

#### **Объяснение кода**
1. **Разбор JSON-структуры:**
   - `convert()` – извлекает названия жанров и ключевых слов.
   - `convert_for_cast()` – оставляет только трёх главных актёров.
   - `convert_for_crew()` – выделяет режиссёра фильма.
2. **Удаление пробелов в названиях:**
   - `delete_space()` убирает пробелы в именах актёров, жанров и ключевых слов.
3. **Преобразование даты:**
   - `release_date` конвертируется в формат года.

### **Ожидаемый результат**
- JSON-структуры разобраны, выделены нужные данные.
- Удалены лишние пробелы.
- Поле `release_date` содержит только год.

---

### **Следующий этап**
На следующем этапе будет проведена лемматизация текста описания фильмов и объединение всех признаков в единое представление (`tags`).


### **Этап 3: Лемматизация и объединение признаков**

#### **Описание**
На данном этапе проводится предобработка текстового описания (`overview`), включая:
- удаление стоп-слов,
- приведение слов к их начальной форме (лемматизация),
- объединение всех ключевых признаков (`overview`, `genres`, `keywords`, `cast`, `crew`) в один тег (`tags`).

#### **Код**
```python
import nltk
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer

nltk.download('stopwords')
nltk.download('wordnet')

stop_words = set(stopwords.words('english'))
lemmatizer = WordNetLemmatizer()

def preprocess_text(text):
    if isinstance(text, str):  
        words = text.lower().split()  
        words = [word for word in words if word not in stop_words]  
        words = [lemmatizer.lemmatize(word) for word in words] 
        return " ".join(words) 
    return ""

# Применение предобработки к описанию фильмов
movies["overview"] = movies["overview"].apply(preprocess_text)
movies[["overview"]].head()

# Разбиение текстов на списки слов
movies['overview'] = movies['overview'].apply(lambda x: x.split())

# Объединение всех признаков в один список
movies['tags'] = movies['overview'] + movies['genres'] + movies['keywords'] + movies['cast'] + movies['crew']

# Создание финального датасета
finally_df = movies.drop(columns=['overview','genres','keywords','cast','crew'])
finally_df['tags'] = finally_df['tags'].apply(lambda x: " ".join(x))
finally_df.reset_index(drop=True, inplace=True)

# Вывод первых строк
finally_df.head()

# Сохранение обработанного датасета
finally_df.to_csv('finally.csv')
```

#### **Объяснение кода**
1. **Лемматизация и удаление стоп-слов:**
   - Используется `WordNetLemmatizer` для приведения слов к их базовой форме (например, `running` → `run`).
   - Удаляются стоп-слова (например, `the`, `is`, `in`).
2. **Разбиение на слова:**
   - Поле `overview` преобразуется в список слов для дальнейшей обработки.
3. **Объединение признаков:**
   - Объединяются все ключевые текстовые признаки (`overview`, `genres`, `keywords`, `cast`, `crew`) в одно поле `tags`, которое будет использоваться для рекомендаций.
4. **Финальная обработка:**
   - Исключаются старые столбцы (`overview`, `genres`, `keywords`, `cast`, `crew`).
   - Преобразуется список `tags` в строку для дальнейшей векторизации.
   - Итоговый датасет сохраняется в `finally.csv`.

### **Ожидаемый результат**
- `overview` очищен от стоп-слов и лемматизирован.
- Все текстовые признаки объединены в одно поле `tags`.
- Датасет готов к векторизации и вычислению схожести.

---

### **Следующий этап**
На следующем этапе будет проведено векторное представление данных с использованием TF-IDF или других методов для расчёта схожести между фильмами.


### **Этап 4: Векторизация текста и расчет схожести**

#### **Описание**
На данном этапе выполняется преобразование объединенного текстового представления (`tags`) в числовой формат с помощью метода `CountVectorizer`. Далее рассчитывается косинусное сходство между фильмами, что позволит определять их схожесть на основе текстового контента.

#### **Код**
```python
import pandas as pd
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.metrics.pairwise import cosine_similarity

# Загрузка обработанных данных
df = pd.read_csv('finally.csv', index_col=0)
df.head()

# Векторизация текста
cv = CountVectorizer(max_features=5000, stop_words='english')
vector = cv.fit_transform(df['tags']).toarray()
vector.shape

# Вычисление косинусного сходства между фильмами
similarity = cosine_similarity(vector)
similarity
```

#### **Объяснение кода**
1. **Векторизация текста:**
   - Используется `CountVectorizer` с `max_features=5000` для преобразования текста в числовой формат.
   - Исключаются стоп-слова (`stop_words='english'`).
2. **Расчет косинусного сходства:**
   - Применяется `cosine_similarity()` для вычисления близости между векторами фильмов.

### **Ожидаемый результат**
- `tags` преобразованы в числовые векторы.
- Расчитано косинусное сходство между всеми фильмами.

---

### **Следующий этап**
На следующем этапе будет реализована система рекомендаций, использующая рассчитанные показатели схожести.


### **Этап 5: Реализация системы рекомендаций**

#### **Описание**
На этом этапе создаётся функция `recommend()`, которая принимает название фильма и минимальный год выпуска, фильтрует данные по указанному году и находит наиболее похожие фильмы с помощью рассчитанных ранее показателей косинусного сходства.

#### **Код**
```python
# Функция рекомендаций

def recommend(movie, min_year):
    # Получаем индекс введенного фильма
    index = df[df['title'] == movie].index[0]
    
    # Фильтруем фильмы, оставляя только те, у которых release_date >= min_year
    filtered_movies = df[df['release_date'] >= min_year].index.tolist()
    
    # Вычисляем косинусное сходство только для фильмов из списка filtered_movies
    distances = [(i, similarity[index][i]) for i in filtered_movies]
    
    # Сортируем по убыванию сходства
    distances = sorted(distances, reverse=True, key=lambda x: x[1])
    
    print(f"Рекомендации для {movie} с годом выпуска от {min_year}:")
    for i in distances[1:4]:  # Пропускаем первый, т.к. это сам фильм
        title = df.iloc[i[0]].title
        year = df.iloc[i[0]].release_date
        score = round(i[1] * 100, 2)  
        print(f"{title} ({year}) - {score}% схожести")

# Пример вызова функции
recommend('Spider-Man', 0)
```

#### **Объяснение кода**
1. **Фильтрация по году выхода:**
   - Исключаются фильмы, вышедшие до указанного минимального года.
2. **Поиск схожих фильмов:**
   - Для заданного фильма вычисляется косинусное сходство с другими фильмами.
   - Фильмы сортируются по убыванию схожести.
3. **Вывод результатов:**
   - Отображаются три наиболее похожих фильма (кроме самого выбранного фильма).

### **Ожидаемый результат**
- Функция принимает название фильма и минимальный год выпуска.
- Выводятся три наиболее схожих фильма с процентом схожести.

---

### **Заключение**
На данном этапе завершена реализация системы рекомендаций. Теперь можно протестировать модель на различных фильмах, а также оптимизировать её, например, используя более сложные модели векторизации (TF-IDF, Word2Vec) или комбинированные подходы.

